博文地址：
http://blog.csdn.net/u014540717/article/details/53193433

YOLO源码详解（三）- 前向传播（forward）
博文地址：
http://blog.csdn.net/u014540717/article/details/53232426

//首先我们看一下list，section, node, kvp这四个结构体
typedef struct{
    int size;
    node *front;
    node *back;
} list;

typedef struct{
    char *type;
    list *options;
} section;

typedef struct node{
    void *val;
    struct node *next;
    struct node *prev;
} node;

typedef struct{
    char *key;
    char *val;
    int used;
} kvp;

//再看下yolo.cfg文件
[net]
batch=64
subdivisions=64
height=448
width=448
channels=3
momentum=0.9
decay=0.0005

learning_rate=0.001
policy=steps
steps=200,400,600,20000,30000
scales=2.5,2,2,.1,.1
max_batches = 40000

[crop]
crop_width=448
crop_height=448
flip=0
.
.
.
/*用矩形表示list，椭圆表示section，圆形表示node，六边形表示kvp，
为了表达方便，我就把section和kvp放到了node里面


２、加载权重函数：load_weights(&net, weightfile)

void load_weights(network *net, char *filename)
{
    //调用load_weights_upto(net, filename, net->n)函数
    load_weights_upto(net, filename, net->n);
}

void load_weights_upto(network *net, char *filename, int cutoff)
{
#ifdef GPU
    if(net->gpu_index >= 0){
        cuda_set_device(net->gpu_index);
    }
#endif
    fprintf(stderr, "Loading weights from %s...", filename);
    //fflush()函数冲洗流中的信息，该函数通常用于处理磁盘文件。
    fflush(stdout);
    FILE *fp = fopen(filename, "rb");
    if(!fp) file_error(filename);

    int major;
    int minor;
    int revision;
    /*size_t fread ( void *buffer, size_t size, size_t count, FILE *stream)
    fread是一个函数。从一个文件流中读数据，最多读取count个项，每个项size个字节，如果调用成功返回实
    际读取到的项个数（小于或等于count），如果不成功或读到文件末尾返回0。*/
    fread(&major, sizeof(int), 1, fp);
    fread(&minor, sizeof(int), 1, fp);
    fread(&revision, sizeof(int), 1, fp);
    fread(net->seen, sizeof(int), 1, fp);
    int transpose = (major > 1000) || (minor > 1000);

    int i;
    //cutoff顾名思义，之后的参数不加载，fine tuning的时候用
    for(i = 0; i < net->n && i < cutoff; ++i){
        //读取各层权重
        layer l = net->layers[i];
        if (l.dontload) continue;
        if(l.type == CONVOLUTIONAL){
            load_convolutional_weights(l, fp);
        }
        //我们看下卷积层的权重加载
		
void load_convolutional_weights(layer l, FILE *fp)
{
    if(l.binary){
        //load_convolutional_weights_binary(l, fp);
        //return;
    }
    //卷积层的参数个数，卷积核个数×通道数×卷积核长度×卷积核宽度
    int num = l.n*l.c*l.size*l.size;
    //这个估计没用上吧，直接给０了
    if(0){
        fread(l.biases + ((l.n != 1374)?0:5), sizeof(float), l.n, fp);
        if (l.batch_normalize && (!l.dontloadscales)){
            fread(l.scales + ((l.n != 1374)?0:5), sizeof(float), l.n, fp);
            fread(l.rolling_mean + ((l.n != 1374)?0:5), sizeof(float), l.n, fp);
            fread(l.rolling_variance + ((l.n != 1374)?0:5), sizeof(float), l.n, fp);
        }
        fread(l.weights + ((l.n != 1374)?0:5*l.c*l.size*l.size), sizeof(float), num, fp);
    }else{
        fread(l.biases, sizeof(float), l.n, fp);
        //如果使用了Batch Normalizationn(https://www.zhihu.com/question/38102762,
                                    　　https://arxiv.org/abs/1502.03167)，
        那就加载三个参数
        if (l.batch_normalize && (!l.dontloadscales)){
            fread(l.scales, sizeof(float), l.n, fp);
            fread(l.rolling_mean, sizeof(float), l.n, fp);
            fread(l.rolling_variance, sizeof(float), l.n, fp);
        }
        fread(l.weights, sizeof(float), num, fp);
    }
    if(l.adam){
        fread(l.m, sizeof(float), num, fp);
        fread(l.v, sizeof(float), num, fp);
    }
    //if(l.c == 3) scal_cpu(num, 1./256, l.weights, 1);
    if (l.flipped) {
        transpose_matrix(l.weights, l.c*l.size*l.size, l.n);
    }
    //if (l.binary) binarize_weights(l.weights, l.n, l.c*l.size*l.size, l.weights);
#ifdef GPU
    if(gpu_index >= 0){
        push_convolutional_layer(l);
    }
#endif
}		
		
